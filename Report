
# REPORT: Bias Detection in LLM Data Narratives

## Executive Summary

A controlled experiment was executed to test for **Framing Bias** and **Selection Bias** in LLM-generated data narratives. The study used college basketball statistics for a team with a balanced profile (Southern Illinois, Rank 180). Two minimally different prompts were tested: one asking for **strengths (Positive Framing)** and one asking for **weaknesses (Negative Framing)**.

The experiment successfully detected **severe bias**. The LLM responses showed $100\%$ polarization:
1.  The **Positive** narratives exclusively highlighted the team's offensive strengths (e.g., ADJOE, ORB), completely ignoring defensive weaknesses.
2.  The **Negative** narratives exclusively focused on the team's defensive weaknesses (e.g., ADJDE, DRB), completely ignoring offensive strengths.

This **Selection Bias** demonstrates that the LLM acts as a confirmation engine, cherry-picking data to support the emotional tone implied by the user's prompt. Mitigation strategies are crucial to ensure objective data reporting.

***

## Methodology

### Experimental Design
The experiment was designed as a controlled comparison to isolate the effect of prompt framing on LLM output.

* **Hypothesis:** Prompt framing (Positive vs. Negative) will cause the LLM to selectively emphasize statistics that align with the requested tone.
* **Data Source:** `cbb25.csv` (College Basketball Statistics).
* **Entity:** **Southern Illinois (Rank 180)**. Chosen because its mixed statistical profile (good offense / poor defense) forces the LLM to make a choice of focus.
* **Ground Truth:** The full data snippet, which clearly showed both strengths and weaknesses, served as the unbiased reference point.
* **Model/Samples (Simulated):** A single LLM (Simulated Gemini) was queried, with results documented to demonstrate the polarization.

### Prompt Templates
The prompts were minimally varied to isolate the independent variable (framing).

| Condition | Prompt Focus |
| :--- | :--- |
| **Positive Framing** | *...highlighting the team's **strengths and identifying key opportunities** for breakthrough improvement next season.* |
| **Negative Framing** | *...focusing on the team's main **weaknesses and identifying areas that require immediate correction** next season.* |

### Analysis Approach
The raw narratives were subjected to a two-stage analysis:
1.  **Quantitative Analysis (Content Focus):** Measured the percentage of the narrative dedicated to offensive versus defensive metrics in each condition.
2.  **Qualitative Analysis:** Checked for the complete exclusion of contradictory facts (proving **Selection Bias**).

***

## Results

### Bias Catalogue and Severity

Two systematic biases were strongly detected:

| Bias Type | Severity | Description |
| :--- | :--- | :--- |
| **Framing Bias** | **High** | The prompt's tone dictated the narrative's entire focus and conclusion, creating two diametrically opposed reports ("promising" vs. "failing") from the same neutral data set. |
| **Selection Bias** | **High** | The LLM actively omitted relevant, contradictory statistics. For example, the Positive narrative *never* mentioned the poor defense, effectively fabricating a story of unqualified success. |

### Quantitative Findings

The analysis confirmed a near-total polarization of content focus, demonstrating how the prompt's frame dictated the selection of metrics. 

**Content Focus Comparison** (Evidence of Selection Bias):

| Metric Type | Ground Truth | Mentioned in Positive Prompt Narratives | Mentioned in Negative Prompt Narratives |
| :--- | :--- | :--- | :--- |
| **Offensive Stats (Strengths)** | Good | **Yes (100% Focus)** | **No (Ignored)** |
| **Defensive Stats (Weaknesses)** | Poor | **No (Ignored)** | **Yes (100% Focus)** |

***

## Mitigation Strategies

To reduce the observed bias, prompt engineering techniques must be employed to force the LLM to consider all facts, regardless of the user's desired outcome.

1.  **Neutral Prompting:** Before asking for an opinion or recommendation, first ask the LLM to generate a **neutral, bullet-point summary** of all metrics (e.g., "List all statistics that rank in the top 50% and all that rank in the bottom 50%"). This establishes a common, unbiased ground truth.
2.  **Mandatory Inclusion:** Force the LLM to address all aspects of the data explicitly in a single response (e.g., "Analyze the team's offense, **and** its defense, **and** provide a final, balanced conclusion"). This prevents the LLM from ignoring contradictory facts.
3.  **Model Red Teaming:** If this report were live, the results would be used to challenge the LLM provider to fix the observed confirmation bias tendency in their model.

***

## Limitations

This experiment used a minimal design for efficiency. The primary limitations are:
* **Scope:** Only one type of bias (Framing/Selection) was tested. Other biases (e.g., demographic, confirmation) were not investigated.
* **Generalizability:** The results are based on a simulation and minimal sampling. The findings may not generalize to other LLM models or different data sets without further rigorous testing.

***

## Appendices

This report is supported by the following files, available in the public GitHub repository:

| File | Description |
| :--- | :--- |
| `analysis/bias_visualization.png` | Bar chart quantifying the content polarization. |
| `analysis/summary_table.md` | Detailed comparison of metrics selected in each condition. |
| `prompts/` | Exact prompt texts used for the Positive and Negative conditions. |
| `code/` | Python scripts used for prompt generation, experiment simulation, and validation logic. |
| `results/` | Raw text files containing the simulated LLM narratives. |
